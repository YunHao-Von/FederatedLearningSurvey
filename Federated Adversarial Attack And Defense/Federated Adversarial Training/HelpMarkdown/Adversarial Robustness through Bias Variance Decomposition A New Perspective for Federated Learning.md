# è®ºæ–‡é˜…è¯»ç¬”è®°:Adversarial Robustness through Bias Variance Decomposition: A New Perspective for Federated Learning  
# è”é‚¦å­¦ä¹ æ–°è§†è§’:åŸºäºåå·®æ–¹å·®åˆ†è§£çš„å¯¹æŠ—é²æ£’æ€§  

## Abstract  
In this work, we show that this paradigm might inherit the adversarial vulnerability of the centralized neural network, i.e., it has deteriorated performance on adversarial examples when the model is deployed.  
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜è¿™ç§èŒƒå¼å¯èƒ½ç»§æ‰¿é›†ä¸­å¼ç¥ç»ç½‘ç»œçš„å¯¹æŠ—æ€§è„†å¼±æ€§ï¼Œå³å½“éƒ¨ç½²æ¨¡å‹æ—¶ï¼Œå®ƒåœ¨å¯¹æŠ—æ€§æ ·æœ¬ä¸Šçš„æ€§èƒ½ä¼šä¸‹é™ã€‚  
To solve this problem, we propose an adversarially robust federated learning framework, named Fed_BVA, with improved server and client update mechanisms. This is motivated by our observation that the generalization error in federated learning can be naturally decomposed into the bias and variance triggered by multiple clients' predictions. Thus, we propose to generate the adversarial examples via maximizing the bias and variance during server update, and learn the adversarially robust model updates with those examples during client update.  
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯¹æŠ—é²æ£’çš„è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºFed_BVAï¼Œå®ƒå…·æœ‰æ”¹è¿›çš„æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯æ›´æ–°æœºåˆ¶ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè”é‚¦å­¦ä¹ ä¸­çš„æ³›åŒ–è¯¯å·®å¯ä»¥è‡ªç„¶åœ°åˆ†è§£ä¸ºç”±å¤šä¸ªå®¢æˆ·é¢„æµ‹è§¦å‘çš„åå·®å’Œæ–¹å·®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºåœ¨æœåŠ¡å™¨æ›´æ–°æœŸé—´é€šè¿‡æœ€å¤§åŒ–åå·®å’Œæ–¹å·®æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œå¹¶åœ¨å®¢æˆ·ç«¯æ›´æ–°æœŸé—´ä½¿ç”¨è¿™äº›æ ·æœ¬è¿›è¡Œå¯¹æŠ—è®­ç»ƒã€‚  

## 1.Introduction  
Our work studies the adversarial robustness of federated learning paradigm by investigating the generalization error incurred in the server's aggregation process from the perspective of bias-variance decomposition.Specifically, we show that this generalization error on the central server can be decomposed as the combination of bias (triggered by the main prediction of these clients) andvariance (triggered by the variations among clients' predictions).This motivates us to propose a novel adversarially robust federated learning framework Fed_BVA1. The key idea is to perform the local robust training on clients by supplying them with bias-variance perturbed examples generated from a tiny auxiliary training set on the central server.  
æˆ‘ä»¬çš„å·¥ä½œä»åå·®-æ–¹å·®åˆ†è§£çš„è§’åº¦ç ”ç©¶äº†æœåŠ¡å™¨èšåˆè¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ³›åŒ–è¯¯å·®ï¼Œç ”ç©¶äº†è”é‚¦å­¦ä¹ èŒƒå¼çš„å¯¹æŠ—é²æ£’æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œä¸­å¤®æœåŠ¡å™¨ä¸Šçš„è¿™ç§æ³›åŒ–è¯¯å·®å¯ä»¥åˆ†è§£ä¸ºåå·®(ç”±è¿™äº›å®¢æˆ·ç«¯çš„ä¸»è¦é¢„æµ‹å¼•èµ·)å’Œæ–¹å·®(ç”±å®¢æˆ·ç«¯é¢„æµ‹ä¹‹é—´çš„ä¸åŒå¼•èµ·)çš„ç»„åˆã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯¹æŠ—é²æ£’çš„è”é‚¦å­¦ä¹ æ¡†æ¶Fed_BVAã€‚å…³é”®æ€æƒ³æ˜¯é€šè¿‡å‘å®¢æˆ·ç«¯æä¾›ç”±ä¸­å¤®æœåŠ¡å™¨ä¸Šçš„ä¸€ä¸ªå°è¾…åŠ©è®­ç»ƒé›†ç”Ÿæˆçš„åå·®æ–¹å·®å¯¹æŠ—æ ·æœ¬è¿›è¡Œå±€éƒ¨å¯¹æŠ—è®­ç»ƒã€‚  

+ First, it encourages the clients to consistently produce the optimal prediction for perturbed examples, thereby leading to a better generalization performance.  
+ Second, local adversarial training on the perturbed examples learns a robust local model, and thus an adversarially robust global model could be aggregated from clients' local updates.    

### Major Contributions  
+ We provide the exact solution of bias-variance analysis w.r.t. the generalization error for neural networks in the federated learning setting. As a comparison, performing the adversarial training on conventional federated learning methods can only focus on the bias of the central model but ignore the variance.  
+ We demonstrate that the conventional federated learning framework is vulnerable to strong attacks with increasing communication rounds even if the adversarial training using the locally generated adversarial examples is performed on each client.  
+ Without violating the clients' privacy, we show that providing a tiny amount of bias-variance perturbed data from the central server to the clients through asymmetrical communication could dramatically improve the robustness of the training model under various adversarial settings.  
+ æˆ‘ä»¬æä¾›äº†ç¥ç»ç½‘ç»œåœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸‹æ³›åŒ–è¯¯å·®çš„æ–¹å·®åå·®åˆ†æçš„ç²¾ç¡®è§£ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨ä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ æ–¹æ³•ä¸Šè¿›è¡Œå¯¹æŠ—æ€§è®­ç»ƒï¼Œåªèƒ½å…³æ³¨ä¸­å¿ƒæ¨¡å‹çš„åå·®ï¼Œè€Œå¿½ç•¥äº†æ–¹å·®ã€‚ 
+ æˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿åœ¨æ¯ä¸ªå®¢æˆ·ç«¯ä¸Šä½¿ç”¨æœ¬åœ°ç”Ÿæˆçš„å¯¹æŠ—æ€§ç¤ºä¾‹è¿›è¡Œå¯¹æŠ—æ€§è®­ç»ƒï¼Œéšç€é€šä¿¡æ¬¡æ•°çš„å¢åŠ ï¼Œä¼ ç»Ÿçš„è”é‚¦å­¦ä¹ æ¡†æ¶ä¹Ÿå®¹æ˜“å—åˆ°å¯¹æŠ—æ”»å‡»ã€‚  
+ åœ¨ä¸ä¾µçŠ¯å®¢æˆ·éšç§çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¡¨æ˜é€šè¿‡ä¸å¯¹ç§°é€šä¿¡ä»ä¸­å¤®æœåŠ¡å™¨å‘å®¢æˆ·ç«¯æä¾›å°‘é‡åå·®æ–¹å·®æ‰°åŠ¨æ•°æ®å¯ä»¥æ˜¾ç€æé«˜è®­ç»ƒæ¨¡å‹åœ¨å„ç§å¯¹æŠ—è®¾ç½®ä¸‹çš„é²æ£’æ€§ã€‚  

## 2.Related Work  
### 2.2 Bias-Variance Decomposition   
Bias-Varianceåˆ†è§£æœ€åˆæ˜¯ä¸ºäº†åˆ†æå­¦ä¹ ç®—æ³•çš„æ³›åŒ–è¯¯å·®è€Œå¼•å…¥çš„ã€‚ç„¶åï¼Œåœ¨å…è®¸çµæ´»æŸå¤±å‡½æ•°çš„åˆ†ç±»ä»»åŠ¡è®¾ç½®ä¸‹ï¼Œäººä»¬ç ”ç©¶äº†å¹¿ä¹‰Bias-Varianceåˆ†è§£ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ä¸€èˆ¬æ¡ä»¶ä¸‹ï¼Œå­¦ä¹ ç®—æ³•çš„æ³›åŒ–è¯¯å·®å¯ä»¥åˆ†è§£ä¸ºåå·®ï¼Œæ–¹å·®å’Œå™ªå£°ã€‚å…¶ä¸­ï¼ŒBiasåº¦é‡äº†ä¸åŒè®­ç»ƒæ•°æ®é›†ä¸Šçš„è®­ç»ƒæ¨¡å‹åå·®æœ€ä¼˜æ¨¡å‹çš„ç¨‹åº¦ï¼ŒVarianceåº¦é‡äº†è¿™äº›è®­ç»ƒæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œå…ˆå‰è§‚å¯Ÿåˆ°ï¼Œéšç€æ¨¡å‹å¤æ‚åº¦çš„å¢åŠ ï¼Œbiaså•è°ƒå‡å°‘ï¼Œvarianceå•è°ƒå¢åŠ ã€‚è¿™è¡¨æ˜æ›´å¥½åœ°æƒè¡¡Biaså’ŒVarianceå¯ä»¥æé«˜å­¦ä¹ ç®—æ³•çš„æ³›åŒ–æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿‘å¹´æ¥çš„ç»éªŒè¡¨æ˜ï¼Œå¢åŠ æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¨¡å‹å¤æ‚æ€§å¾€å¾€ä¼šäº§ç”Ÿæ›´å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œè¿™ä¸ä»¥å¾€çš„åå·®æ–¹å·®åˆ†ææ˜¯çŸ›ç›¾çš„ã€‚æ ¹æ®è¿™ä¸€æƒ³æ³•ï¼Œåå·®-æ–¹å·®æƒè¡¡åœ¨ç°ä»£ç¥ç»ç½‘ç»œæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒç ”ç©¶ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€æ¨¡å‹å¤æ‚åº¦çš„å¢åŠ ï¼ŒVarianceæ›´æœ‰å¯èƒ½æ˜¯å…ˆå¢å¤§å†å‡å°ã€‚
æˆ‘ä»¬æƒ³æŒ‡å‡ºçš„æ˜¯ï¼Œä¸æ ‡å‡†çš„ç›‘ç£å­¦ä¹ ç›¸æ¯”ï¼Œè”é‚¦å­¦ä¹ å¯ä»¥æ›´å¥½çš„é€šè¿‡bias-varianceæ¥è¡¨å¾ã€‚è¿™æ˜¯å› ä¸ºåœ¨æœ¬åœ°å®¢æˆ·ä¸­è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥è‡ªç„¶åœ°åº”ç”¨äºå®šä¹‰å­¦ä¹ ç®—æ³•æ³›åŒ–è¯¯å·®ä¸­çš„åå·®å’Œè¯¯å·®ã€‚  

### 3.Preliminaries  
In this section, we formally present the problem definition and the bias-variance trade-off in the classification setting.  
### 3.1 Federated learning   
åœ¨è”é‚¦å­¦ä¹ ä¸­ï¼Œæœ‰ä¸€ä¸ªä¸­å¤®æœåŠ¡å™¨å’ŒKä¸ªå®¢æˆ·ç«¯ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯éƒ½æœ‰æ•°æ®é›†$D_k = \{(x^k_i,t^k_i)  \} ^{n_k}_{i=1}$. $x^k_i,t^k_i,n_k$åˆ†åˆ«ä»£è¡¨ç‰¹å¾ï¼Œæ ‡ç­¾ï¼Œæ•°é‡ã€‚æ¯ä¸ªæ•°æ®é›†$D_k$åªè¢«ç¬¬kä¸ªå®¢æˆ·ç«¯æ‰€æœ‰ï¼Œä¸ä¼šè¢«åˆ†äº«åˆ°ä¸­å¤®æœåŠ¡å™¨å’Œå…¶ä»–å®¢æˆ·ç«¯ä¸Šã€‚   
è”é‚¦å­¦ä¹ çš„ä¸€èˆ¬æ­¥éª¤:  
+ Client Update: Each client updates local parameters $ğ‘¤_ğ‘˜$ by minimizing the empirical loss over its own training set;  
+ Forward Communication:Each client uploads its parameter update to the central server;  
+ Server Update: It synchronously aggregates the received parameters;  
+ Backward Communication: The global parameters are sent back to the clients.  

### 3.2 Problem Definition and Motivation  
Adversarially robust federated learning aims to output a trained model on the central server that is robust against adversarial perturbations on the test set $D_{ğ‘¡ğ‘’ğ‘ ğ‘¡}$ .  

The generalization error of the server's model is induced by both bias and variance of local clients' models.we propose to analyze the adversarially robust federated learning in a unified framework.  
æœåŠ¡å™¨æ¨¡å‹çš„æ³›åŒ–è¯¯å·®æ˜¯ç”±æœ¬åœ°å®¢æˆ·ç«¯æ¨¡å‹çš„åå·®å’Œæ–¹å·®å…±åŒå¼•èµ·çš„,æˆ‘ä»¬å»ºè®®åœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­åˆ†æå¯¹æŠ—é²æ£’è”é‚¦å­¦ä¹ ã€‚  

The crucial idea is to generate some global adversarial examples shared with clients by leveraging a tiny auxiliary training set $D_s$ with $n_s$ examples on the server.  
å…³é”®æ€æƒ³æ˜¯é€šè¿‡åˆ©ç”¨ä¸€ä¸ªå°çš„è¾…åŠ©è®­ç»ƒé›†ï¼Œç”Ÿæˆä¸€äº›ä¸å®¢æˆ·å…±äº«çš„å…¨å±€å¯¹æŠ—æ€§ç¤ºä¾‹

The problem definition has the following properties:  
+ Asymmetrical communication: The asymmetrical communication between each client and server cloud is allowed: the server provides both global model parameters and limited shared data to the clients; while each client uploads its local model parameters back to the server.  
+ ä¸å¯¹ç§°é€šä¿¡ï¼šå…è®¸æ¯ä¸ªå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨äº‘ä¹‹é—´çš„ä¸å¯¹ç§°é€šä¿¡ï¼šæœåŠ¡å™¨å‘å®¢æˆ·ç«¯æä¾›å…¨å±€æ¨¡å‹å‚æ•°å’Œæœ‰é™çš„å…±äº«æ•°æ®ï¼›è€Œæ¯ä¸ªå®¢æˆ·ç«¯å°†å…¶æœ¬åœ°æ¨¡å‹å‚æ•°ä¸Šä¼ å›æœåŠ¡å™¨ã€‚  
+ Data distribution: All training examples on the clients and the server are assumed to follow the same data distribution. However, the experiments show that our proposed algorithm also achieves outstanding performance under the non-IID setting (see Subsection 6.2), which could be commonly seen among personalized clients in real scenarios.  
+ æ•°æ®åˆ†å¸ƒï¼šå‡è®¾å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¸Šçš„æ‰€æœ‰è®­ç»ƒå®ä¾‹éƒ½éµå¾ªç›¸åŒçš„æ•°æ®åˆ†å¸ƒã€‚ç„¶è€Œï¼Œå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ç®—æ³•åœ¨éIIDç¯å¢ƒä¸‹ä¹Ÿå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½(è§ç¬¬6.2èŠ‚)ï¼Œè¿™åœ¨çœŸå®åœºæ™¯ä¸­çš„ä¸ªæ€§åŒ–å®¢æˆ·ä¸­æ˜¯å¸¸è§çš„ã€‚  
+ Shared learning algorithm: All the clients are assumed to use the identical model ğ‘“ (Â·), including architectures as well as hyper-parameters.  
+ å…±äº«å­¦ä¹ ç®—æ³•ï¼šå‡è®¾æ‰€æœ‰å®¢æˆ·ç«¯ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹f(Â·)ï¼ŒåŒ…æ‹¬æ¨¡å‹ç»“æ„å’Œè¶…å‚æ•°ã€‚  

### 3.3 Bias-Variance Trade-off  
![1684121203730](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684121203730.png)  
![1684121238373](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684121238373.png)  
![1684121304612](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684121304612.png)   

## 4.The Proposed Framework  
In this section, we present the adversarially robust federated learning framework. It follows the same paradigm of traditional federated learning (see Subsection 3.1) but with substantial modifications on the server update and client update as follows.  

### 4.1 Server Update with Data Poisoning  
This Server has tow crucial components:  
+ Model Aggregation: It synchronously compresses and aggregates the received local model parameters.  
+ Adversarial Examples Producing: This component is designed to produce adversarially perturbed examples which are induced by a poisoning attack algorithm for the usage of adversarial training.  

#### 4.1.1 Model Aggregation   
Our framework is flexible to be incorporated with existing model aggregation methods, FedAvg, FedMA, AFL, etc.  

#### 4.1.2 Adversarial Examples   
å¦‚æœæˆ‘ä»¬ç›´æ¥åœ¨æ¯ä¸ªå®¢æˆ·ç«¯ä¸Šè¿›è¡Œå¯¹æŠ—è®­ç»ƒï¼Œä¼šé€ æˆä»¥ä¸‹ä¸¤ä¸ªé—®é¢˜:  
+ First, it will significantly increase the computational burden and memory usage on local clients.  
+ Second, the locally generated adversarial examples make the augmented data distributions of local clients much more biased, which challenges the standard server-level aggregation mechanisms.  

å®šç†4.1  
åœ¨äºŒå…ƒæƒ…å†µä¸‹ï¼ŒEq.(1)ä¸­çš„åˆ†è§£é€‚ç”¨äºä»»ä½•æ»¡è¶³çš„$\forall_y L(y,y)=0ä¸”\forall_{y_1 \ne y_2}L(y_1,y_2) \ne 0$å®å€¼æŸå¤±å‡½æ•°ï¼Œä¸”$\lambda$æ»¡è¶³:  
![1684133082230](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684133082230.png)  
<font color =red>
when bias exists $ğ‘¦_ğ‘š\ne ğ‘¦_âˆ—$, a negative variance will help to reduce error when the prediction is identical to the main prediction or optimal prediction  
</font>  
ä½†æ˜¯æœ¬æ–‡ä¸­çš„$\lambda$è¢«è§†ä¸ºè¶…å‚æ•°ã€‚ 

å¦‚ä½•ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œå‚è€ƒç­‰å¼(1)ï¼Œæå‡ºæœ€ä¼˜åŒ–ä¸‹å¼æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬:  
![1684133826504](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684133826504.png)   

### 4.2 Robust Clinet Update  
The robust training of one client's prediction model ($ğ‘¤_ğ‘˜ $) can be formulated as the following minimization problem:  
![1684134221168](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684134221168.png)  

Bias measures the systematic loss of a learning algorithm, and the variance measures the prediction consistency of the learner over different training sets.  
åå·®è¡¡é‡å­¦ä¹ ç®—æ³•çš„ç³»ç»ŸæŸå¤±ï¼Œæ–¹å·®è¡¡é‡å­¦ä¹ è€…åœ¨ä¸åŒè®­ç»ƒé›†ä¸Šçš„é¢„æµ‹ä¸€è‡´æ€§ã€‚  

our robust federated learning framework has the following advantages:  
+ it encourages the clients to consistently produce the optimal prediction for perturbed examples, thereby leading to a better generalization performance;  
+ local adversarial training on perturbed examples allows to learn a robust local model, and thus a robust global model could be aggregated from clients.  

## 5.Instantiated Algorithm  
### 5.1 Bias-Variance Attack  
#### Bias-variance based Fast Gradient Sign Method (BV-FGSM)  
![1684146409776](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684146409776.png)  
#### Bias-variance based Projected Gradient Descent (BV-PGD)  
![1684146445148](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684146445148.png)  

The proposed framework could b generalized to any gradientbased adversarial attack algorithms where the gradients of bias ğµ(Â·)and variance ğ‘‰ (Â·) w.r.t. ğ‘¥ are tractable when estimated from finite training sets. Compared with the existing attack methods, our loss function the adversary aims to optimize is a linear combination of bias and variance, whereas existing work mainly focused on attacking the overall classification error involving bias only.  

ä¸‹åˆ—å®šç†è¡¨æ˜ï¼Œbiaså’Œvarianceï¼Œä»¥åŠå®ƒä»¬åœ¨input xä¸Šçš„æ¢¯åº¦ï¼Œéƒ½å¯ä»¥é€šè¿‡client modelæ¥è¿›è¡Œä¼°è®¡ã€‚  
å®šç†5.1:  
å‡è®¾Læ˜¯äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç»éªŒä¼°è®¡çš„å¯¹ä¸€ä¸ªè¾“å…¥æ ·æœ¬$(x,t)$çš„main prediction $y_m$, $y_m(x) = \frac{1}{K}\sum_{k=1}^K f_{D_k}(x;w_k)$,![1684147336458](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684147336458.png),![1684147347402](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684147347402.png)ã€‚ä»–ä»¬å¯¹åº”æ¢¯åº¦ä¸º:  
![1684147382490](image/AdversarialRobustnessthroughBiasVarianceDecompositionANewPerspectiveforFederatedLearning/1684147382490.png)  

### 5.2 Fed_BVA  
