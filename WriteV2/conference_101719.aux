\relax 
\citation{b1}
\citation{b2}
\citation{b3}
\citation{b4}
\citation{b5,b6}
\citation{b7,b8}
\citation{b9,b10}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The server firstly sends a global model to all selected clients as local models. These clients then use their local datasets to train the local models and upload their trained model updates to the central server. After receiving the updates from all selected clients, the server updates the global model by averaging the uploaded updates. Throughout the training process, only the data owners have access to their local data. Finally, the server sends the updated global model to the selected clients.}}{1}{}\protected@file@percent }
\newlabel{fig1}{{1}{1}}
\citation{b11,b12,b13,b14,b15}
\citation{b16,b17,b18,b19,b20}
\citation{b21,b22,b27}
\citation{b23,b24,b25,b26}
\@writefile{toc}{\contentsline {section}{\numberline {II}Threat Model}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Backdoor attacks refer to a malicious backdoor added to the global model by malicious participants during training process. The backdoor can be triggered by specific inputs, allowing attackers to control the outputs of the model. The goal of the backdoor attacks is to make the model maintain the normal outputs on the normal samples, but the outputs expected by attackers are shown in the backdoor samples.}}{2}{}\protected@file@percent }
\newlabel{fig2}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces In Byzantine attacks, there exist one or more malicious participants in the federated learning system who disrupt the training process by sending incorrect or misleading updates to the central server, causing abnormal convergence}}{2}{}\protected@file@percent }
\newlabel{fig3}{{3}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Adversarial attacks require adding subtle and carefully crafted perturbations to the input data to deceive the model and cause it to make incorrect predictions. }}{2}{}\protected@file@percent }
\newlabel{fig4}{{4}{2}}
\citation{b28}
\citation{b29}
\citation{b23,b24}
\citation{b30}
\citation{b31}
\citation{b21}
\citation{b47}
\citation{b48}
\citation{b49}
\citation{b50}
\citation{b51}
\citation{b51}
\citation{b52}
\citation{b53}
\citation{b24}
\citation{b27}
\citation{b59}
\citation{b60}
\citation{b61}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Training Phase and Inference Phase}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}1}Training Phase}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}2}Inference Phase}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Untargeted and Targeted}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-B}1}Untargeted attacks}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-B}2}Targeted Attacks}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Backdoor Attack}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Data Poisoning Attack}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}1}Visible Poisoning}{3}{}\protected@file@percent }
\citation{b251}
\citation{b251}
\citation{b64}
\citation{b54}
\citation{b55}
\citation{b56}
\citation{b57}
\citation{b58}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces DBA decomposes a global trigger pattern, similar to a trigger in centralized attack, into local patterns and embeds them into different malicious clients.}}{4}{}\protected@file@percent }
\newlabel{fig8}{{5}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {